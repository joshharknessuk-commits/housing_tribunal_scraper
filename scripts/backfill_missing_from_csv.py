"""Backfill missing PDF documents from a CSV export.

Reads a CSV generated by `scripts/find_extra_pdfs.py --csv` (or equivalent),
then inserts any missing PDF URLs into the target documents table while keeping
case IDs aligned with the cases table.

Use `--dry-run` to preview inserts without touching the database.
"""

from __future__ import annotations

import argparse
import csv
import os
import sys
from dataclasses import dataclass
from pathlib import Path
from typing import Iterable
from uuid import UUID

import psycopg
from psycopg import sql

ROOT = Path(__file__).resolve().parents[1]
if str(ROOT) not in sys.path:
    sys.path.insert(0, str(ROOT))

from scraper.session import build_http_session
from scripts import rescrape_cases


@dataclass
class CsvRow:
    case_id: str
    slug: str
    html_url: str
    db_count: int
    web_count: int
    missing_count: int
    missing_urls: list[str]


def parse_args() -> argparse.Namespace:
    parser = argparse.ArgumentParser(description=__doc__)
    parser.add_argument("--csv", required=True, help="Path to CSV produced by find_extra_pdfs.py")
    parser.add_argument(
        "--documents-table",
        default=os.getenv("DOCUMENTS_TABLE", "dev.documents"),
        help="Destination documents table",
    )
    parser.add_argument(
        "--cases-table",
        default=os.getenv("CASES_TABLE", "dev.cases"),
        help="Cases table (unused but kept for parity)",
    )
    parser.add_argument("--limit", type=int, help="Only process the first N rows from the CSV")
    parser.add_argument("--offset", type=int, default=0, help="Skip the first N rows from the CSV")
    parser.add_argument("--dry-run", action="store_true", help="Log intended inserts without DB writes")
    parser.add_argument(
        "--download-bytes",
        type=int,
        choices=(0, 1),
        default=0,
        help="Download PDF bytes before insert (stores sha256/mime/bytes)",
    )
    parser.add_argument("--http-timeout", type=float, default=30.0, help="HTTP timeout when downloading PDFs")
    return parser.parse_args()


def quote_table(table: str) -> sql.Composed:
    parts = table.split(".")
    idents = [sql.Identifier(part) for part in parts]
    if len(idents) == 1:
        return idents[0]
    return sql.SQL(".").join(idents)


def clean_headers(reader: csv.DictReader) -> dict[str, str]:
    assert reader.fieldnames is not None
    return {name: name.strip() for name in reader.fieldnames}


def parse_missing_urls(value: str | None) -> list[str]:
    if not value:
        return []
    cleaned = value.replace("\r", "\n")
    tokens: list[str] = []
    for chunk in cleaned.split("\n"):
        for piece in chunk.split(" \\n"):
            trimmed = piece.strip().strip('"').strip("'")
            if trimmed:
                tokens.append(trimmed)
    return tokens


def is_valid_uuid(value: str | None) -> bool:
    if not value:
        return False
    try:
        UUID(value)
        return True
    except Exception:
        return False


def load_csv(path: str, *, limit: int | None, offset: int) -> Iterable[CsvRow]:
    with open(path, newline="", encoding="utf-8") as handle:
        reader = csv.DictReader(handle)
        header_map = clean_headers(reader)
        rows: list[CsvRow] = []
        current: CsvRow | None = None

        for raw in reader:
            normalized = {
                header_map[key]: (value.strip() if isinstance(value, str) else value)
                for key, value in raw.items()
            }

            case_id = normalized.get("case_id")
            if is_valid_uuid(case_id):
                row = CsvRow(
                    case_id=case_id,
                    slug=normalized.get("govuk_slug", ""),
                    html_url=normalized.get("html_url", ""),
                    db_count=int(normalized.get("db_count", 0) or 0),
                    web_count=int(normalized.get("web_count", 0) or 0),
                    missing_count=int(normalized.get("missing_count", 0) or 0),
                    missing_urls=parse_missing_urls(normalized.get("missing_urls")),
                )
                rows.append(row)
                current = row
                continue

            if current is None:
                continue

            extra_urls: list[str] = []
            for value in normalized.values():
                if isinstance(value, str):
                    extra_urls.extend(parse_missing_urls(value))
            if extra_urls:
                current.missing_urls.extend(extra_urls)

        sliced = rows[offset:]
        if limit is not None:
            sliced = sliced[:limit]
        return sliced


def document_exists(conn: psycopg.Connection, table: sql.Composed, pdf_url: str) -> bool:
    query = sql.SQL("SELECT 1 FROM {} WHERE pdf_url = %s LIMIT 1").format(table)
    with conn.cursor() as cur:
        cur.execute(query, (pdf_url,))
        return cur.fetchone() is not None


def insert_document(
    conn: psycopg.Connection,
    table: sql.Composed,
    *,
    case_id: str,
    pdf_url: str,
    sha256_hex: str | None,
    bytes_len: int | None,
    mime: str | None,
    filename: str | None,
    document_type: str | None,
    classification_method: str,
) -> bool:
    query = sql.SQL(
        """
        INSERT INTO {table} (
          case_id, pdf_url, sha256, bytes, mime, blob_url, filename,
          downloaded_at, processed, document_type, document_classification_method
        )
        VALUES (%s, %s, %s, %s, %s, %s, %s, NOW(), FALSE, %s, %s)
        """
    ).format(table=table)
    with conn.cursor() as cur:
        cur.execute(
            query,
            (
                case_id,
                pdf_url,
                sha256_hex,
                bytes_len,
                mime,
                None,
                filename,
                document_type,
                classification_method,
            ),
        )
        return True


def derive_filename(url: str) -> str | None:
    if not url:
        return None
    parts = url.split("/")
    return parts[-1] if parts else None


def decide_document_type(url: str) -> tuple[str | None, str]:
    lowered = url.lower()
    if "reason" in lowered:
        return "reasons", "filename"
    if "decision" in lowered or "determination" in lowered:
        return "decision", "filename"
    return None, "default"


def main() -> int:
    args = parse_args()
    db_url = os.getenv("DATABASE_URL")
    if not db_url and not args.dry_run:
        raise SystemExit("DATABASE_URL is required (or use --dry-run)")

    docs_table = quote_table(args.documents_table)

    rows = list(load_csv(args.csv, limit=args.limit, offset=args.offset))
    if not rows:
        print("No rows to process from CSV slice.")
        return 0

    print("========================================")
    print("Starting CSV BACKFILL")
    print("========================================")
    print(f"CSV path       : {args.csv}")
    print(f"Documents table: {args.documents_table}")
    print(f"Dry run        : {args.dry_run}")
    print(f"Download bytes : {bool(args.download_bytes)}")
    print(f"Rows selected  : {len(rows)}\n")

    http_session = None
    if args.download_bytes:
        http_session = build_http_session(timeout=args.http_timeout)

    total_urls = 0
    already_present = 0
    inserted = 0
    errors = 0

    connection = None
    if db_url:
        connection = psycopg.connect(db_url)
        connection.execute("SET SESSION statement_timeout = '5min';")
    elif args.dry_run:
        print("⚠️  DATABASE_URL not set; skipping existence checks (dry-run mode)")

    try:
        for row in rows:
            if not row.missing_urls:
                continue
            print(f"Case {row.case_id} ({row.slug})")
            for url in row.missing_urls:
                total_urls += 1

                exists = False
                if connection is not None:
                    exists = document_exists(connection, docs_table, url)

                if exists:
                    already_present += 1
                    print(f"  = exists : {url}")
                    continue

                filename = derive_filename(url)
                document_type, classification_method = decide_document_type(url)
                sha_hex = None
                bytes_len = None
                mime = None

                if args.dry_run:
                    print(f"  + would insert: {url}")
                    continue

                try:
                    if args.download_bytes:
                        assert http_session is not None
                        buf, bytes_len, mime = rescrape_cases.download_pdf(url, http_session)
                        sha_hex = rescrape_cases.sha256_bytes(buf)

                    if insert_document(
                        connection,
                        docs_table,
                        case_id=row.case_id,
                        pdf_url=url,
                        sha256_hex=sha_hex,
                        bytes_len=bytes_len,
                        mime=mime,
                        filename=filename,
                        document_type=document_type,
                        classification_method=classification_method,
                    ):
                        inserted += 1
                        print(f"  + inserted: {url}")
                    else:
                        already_present += 1
                        print(f"  = conflict (ignored): {url}")
                except Exception as exc:  # noqa: BLE001
                    errors += 1
                    if connection is not None:
                        connection.rollback()
                    print(f"  ! error inserting {url}: {exc}")
                else:
                    if connection is not None:
                        connection.commit()
    finally:
        if connection is not None:
            connection.close()
        if http_session:
            http_session.close()

    print("\n========================================")
    print("CSV BACKFILL COMPLETE")
    print("========================================")
    print(f"Total URLs seen  : {total_urls}")
    print(f"Inserted         : {inserted}")
    print(f"Already present  : {already_present}")
    print(f"Errors           : {errors}")

    return 0 if errors == 0 else 1


if __name__ == "__main__":
    raise SystemExit(main())
